{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Lior Tondovski, Ilan Vasilevski, Maya Vilenko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import AdaBoost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "#import RandomSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import shap\n",
    "import shap\n",
    "#import AUC and accuracy score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train and test data pickls\n",
    "test_data= pd.read_pickle('processed_files/test_data.pkl')\n",
    "validation_data = pd.read_pickle('processed_files/validation_data.pkl')\n",
    "train_data_undersampled = pd.read_pickle('processed_files/train_data_undersampled.pkl')\n",
    "train_data_undersampled_sm = pd.read_pickle('processed_files/train_data_undersampled_sm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a subset of the data for faster hyperparameter tuning and model training\n",
    "train_data_undersampled_sm = train_data_undersampled_sm.sample(50000, random_state=42)\n",
    "train_data_undersampled = train_data_undersampled.sample(50000, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Random Search\n",
    "\n",
    "##### We chose to train AdaBoost, XGBoost and Random Forest (all tree-based models because most of the time they perform best on tabular data with a lot of categorical features)\n",
    "##### The reason for choosing Random Search is due to the fact it is pretty fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train data to predictors and target\n",
    "X_train_sm = train_data_undersampled_sm.drop('clicked', axis=1)\n",
    "y_train_sm = train_data_undersampled_sm['clicked']\n",
    "X_train = train_data_undersampled.drop('clicked', axis=1)\n",
    "y_train = train_data_undersampled['clicked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "0.8672802531419253\n"
     ]
    }
   ],
   "source": [
    "#ADABoost Random Search \n",
    "ada_clf = AdaBoostClassifier(random_state=12)\n",
    "#The parameters to be tuned\n",
    "ada_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300], # n_estimators is the number of stumps to be used\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3] # learning_rate is the weight of each stump\n",
    "}\n",
    "\n",
    "\n",
    "ada_random_search_sm = RandomizedSearchCV(ada_clf, param_distributions=ada_param_grid, n_iter=5, cv=2, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=12)\n",
    "ada_random_search_sm.fit(X_train_sm, y_train_sm)\n",
    "ada_random_search_sm.best_params_\n",
    "\n",
    "#print the best score\n",
    "print(ada_random_search_sm.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "0.8398484777936833\n"
     ]
    }
   ],
   "source": [
    "#ADABoost Random Search \n",
    "ada_clf = AdaBoostClassifier(random_state=12)\n",
    "\n",
    "ada_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "ada_random_search = RandomizedSearchCV(ada_clf, param_distributions=ada_param_grid, n_iter=5, cv=2, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=12)\n",
    "ada_random_search.fit(X_train, y_train)\n",
    "ada_random_search.best_params_\n",
    "\n",
    "#print the best score\n",
    "print(ada_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Random Search\n",
    "xgb_clf = XGBClassifier(random_state=12)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # n_estimators is the number of weak classifiers to be used\n",
    "    'learning_rate': [0.05, 0.1, 0.3], # learning_rate is the weight of each weak classifier\n",
    "    'max_depth': [3, 4, 5], # max_depth is the maximum depth of each tree\n",
    "    'gamma': [0, 0.5, 1],  # gamma is hyperparameter that controls the tree pruning, higher values of gamma lead to more pruning\n",
    "    'reg_lambda': [0, 0.5, 1] # reg_lambda is the L2 regularization term on weights\n",
    "}\n",
    "\n",
    "xgb_random_search_sm = RandomizedSearchCV(xgb_clf, param_distributions=xgb_param_grid, n_iter=5, cv=2, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=12)\n",
    "xgb_random_search_sm.fit(X_train_sm, y_train_sm)\n",
    "xgb_random_search_sm.best_params_\n",
    "\n",
    "#print the best score\n",
    "print(xgb_random_search_sm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Random Search\n",
    "xgb_clf = XGBClassifier(random_state=12)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.3],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.5, 1],\n",
    "    \n",
    "}\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(xgb_clf, param_distributions=xgb_param_grid, n_iter=5, cv=2, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=12)\n",
    "xgb_random_search.fit(X_train_sm, y_train_sm)\n",
    "xgb_random_search.best_params_\n",
    "\n",
    "print(xgb_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Random Search\n",
    "rf_clf = RandomForestClassifier(random_state=12)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # n_estimators is the number of trees to be used\n",
    "    'max_depth': [3, 4, 5], # max_depth is the maximum depth of each tree\n",
    "    'min_samples_split': [2, 3, 20], # min_samples_split is the minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 10], # min_samples_leaf is the minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "rf_random_search_sm = RandomizedSearchCV(rf_clf, param_distributions=rf_param_grid, n_iter=5, cv=2, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=12)\n",
    "rf_random_search_sm.fit(X_train_sm, y_train_sm)\n",
    "rf_random_search_sm.best_params_\n",
    "\n",
    "print(rf_random_search_sm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Random Search\n",
    "rf_clf = RandomForestClassifier(random_state=12)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 3, 20],\n",
    "    'min_samples_leaf': [1, 2, 10],\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(rf_clf, param_distributions=rf_param_grid, n_iter=5, cv=2, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=12)\n",
    "rf_random_search.fit(X_train_sm, y_train_sm)\n",
    "rf_random_search.best_params_\n",
    "\n",
    "print(rf_random_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model the Achieced the best performance in the ROC-AUC metric is: XGBoost on the data with the synthethic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will use the best parameters to train the model on the entire training set\n",
    "#use XGBClassifier with the best parameters from the random search\n",
    "xgb_clf = XGBClassifier(**xgb_random_search_sm.best_params_)\n",
    "xgb_clf.fit(X_train_sm, y_train_sm)\n",
    "#predict on the validation set\n",
    "X_val = validation_data.drop(['clicked'], axis=1)\n",
    "y_val = validation_data['clicked']\n",
    "y_pred = xgb_clf.predict(X_val)\n",
    "y_pred_prob = xgb_clf.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy score\n",
    "print(f'Accuracy Score : {accuracy_score(y_val, y_pred)}')\n",
    "#calculate the precision score\n",
    "print(f'Precision Score : {precision_score(y_val, y_pred)}')\n",
    "#calculate the recall score\n",
    "print(f'Recall Score : {recall_score(y_val, y_pred)}')\n",
    "#calculate the f1 score\n",
    "print(f'F1 Score : {f1_score(y_val, y_pred)}')\n",
    "#calculate auc\n",
    "print(f'AUC : {roc_auc_score(y_val, y_pred_prob)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the ROC curve\n",
    "plot_roc_curve(y_val, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the precision recall curve\n",
    "plot_precision_recall_curve(y_val, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, it is evident that the model incorrectly identifies non-clicked adds as clicked\n",
    "The reason for it is the fact that the validation data represents the actual ratio of clicked ads."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importnace With Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate the feature importance with shap values\n",
    "explainer = shap.TreeExplainer(xgb_clf)\n",
    "shap_values = explainer.shap_values(X_val)\n",
    "#plot the top 10 features with the highest shap values\n",
    "shap.summary_plot(shap_values, X_val, plot_type='bar', max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 10 features with the beeswarm plot\n",
    "shap.summary_plot(shap_values, X_val, plot_type='dot', max_display=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the shap beeswarm and feature importance, the banner position is the most effective feature. It is very likely that there was no click if the banner was located at the bottom.\n",
    "\n",
    "In addition, app_cat_games - casual attracts more clicks, while app_cat_games is exactly the opposite.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#watterfall plot\n",
    "#show real label and predicted label\n",
    "print(f'Real Label : {y_val.iloc[0]}')\n",
    "print(f'Predicted Label : {y_pred[0]}')\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_val.iloc[0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that example it can be seen that the model chose to classifiy as \"not clicked\", mainly due to the banner position, device model and the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show real label and predicted label\n",
    "print(f'Real Label : {y_val.iloc[1]}')\n",
    "print(f'Predicted Label : {y_pred[1]}')\n",
    "shap.force_plot(explainer.expected_value, shap_values[1,:], X_val.iloc[1,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that example it can be seen that the model chose to classifiy as \"not clicked\", user_isp, the state, month, device model and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get index of the ones\n",
    "index = np.where(y_val == 1)[0]\n",
    "#show real label and predicted label\n",
    "print(f'Real Label: {y_val.iloc[index[0]]}')\n",
    "print(f'Predicted Label : {y_pred[index[0]]}')\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[index[0],:], X_val.iloc[index[0],:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that example, it can be seen that the model chose to classify as \"clicked\", mostly due to the banner position and device model.\n",
    "The device_width, the user_isp, as well as the day and hour, pulled strongly towards a \"not clicked\" and therfore the model misclassified this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test set\n",
    "X_test = test_data\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "#save the predictions to a text file\n",
    "np.savetxt('output_12.txt', y_pred, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0162daef2ac4f91d71dc659d7366b1318efa6dce3a9605ecac659f5b282e8a3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
